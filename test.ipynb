{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count generated trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654\n",
      "651\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "data_path = 'data/json_2.1.0_Recep:original-bert'\n",
    "feat_path = 'data/json_feat_2.1.0'\n",
    "for split in ['valid_seen', 'valid_unseen']:\n",
    "    cnt = 0\n",
    "    for ep in os.listdir(os.path.join(data_path, split)):\n",
    "        if os.path.isfile(os.path.join(data_path, split, ep)):\n",
    "            continue\n",
    "        for trial in os.listdir(os.path.join(data_path, split, ep)):\n",
    "            for file in os.listdir(os.path.join(feat_path, ep, trial, 'pp')):\n",
    "                if file.endswith('.json'):\n",
    "                    cnt += 1\n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare generation fail cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "fail = {}\n",
    "with open('gen/exp/0.3/fail-valid_seen.json') as f:\n",
    "    recep = json.load(f)\n",
    "with open('gen/exp/0.3/fail-valid_seen-pddlMatched.json') as f:\n",
    "    recep_matched = json.load(f)\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "for key in recep_matched:\n",
    "    if key not in recep:\n",
    "        fail.update({key: recep_matched[key]})\n",
    "    \n",
    "total = 0  \n",
    "cnt = 0\n",
    "for trial, error in fail.items():\n",
    "    total += 1\n",
    "    if 'Empty plan' in error['error']:\n",
    "        cnt += 1\n",
    "print(total)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare fail cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이건 시뮬레이터로 실행시켜서 봐야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare generated pddl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "def process_task(raw_tasks):\n",
    "    tasks = []\n",
    "    for t in raw_tasks:\n",
    "        if '\\n' in t:\n",
    "            tasks.extend([_t.replace('\\b', '').strip() for _t in t.split('\\n') if _t != ''])\n",
    "        else:\n",
    "            tasks.append(t.replace('\\b', '').strip())\n",
    "    return tasks\n",
    "\n",
    "with open('gen/scripts/roberta-sentence_match.json', 'r') as f:\n",
    "    sentence_match = json.load(f)\n",
    "with open('gen/scripts/pddl_match.json') as f:\n",
    "    prediction_pddl_match = json.load(f)\n",
    "    \n",
    "log = ''\n",
    "data_path = 'data/json_2.1.0'\n",
    "for split in ['valid_seen', 'valid_unseen']:\n",
    "    for ep in os.listdir(os.path.join(data_path, split)):\n",
    "        if os.path.isfile(os.path.join(data_path, split, ep)):\n",
    "            continue\n",
    "        for trial in os.listdir(os.path.join(data_path, split, ep)):\n",
    "            with open(os.path.join(data_path, split, ep, trial, 'traj_data.json')) as f:\n",
    "                traj_data = json.load(f)\n",
    "            pddl = traj_data['pddl_params']\n",
    "            tasks = process_task([ann['task_desc'] for ann in traj_data['turk_annotations']['anns']])\n",
    "            predicted_pddl = [prediction_pddl_match[t] for t in tasks]\n",
    "            \n",
    "            for i, task in enumerate(tasks):\n",
    "                if pddl == predicted_pddl[i]:\n",
    "                    continue\n",
    "                idx = sentence_match['pred'].index(task)\n",
    "                log += os.path.join(ep, trial)+'\\n'\n",
    "                log += '%20s: %s\\n' % ('valid', task)\n",
    "                log += '%20s: %s\\n' % ('matched train task', sentence_match['gt'][idx])\n",
    "                log += '-'*50 + '\\n'\n",
    "                log += '%15s %15s %15s\\n'%('params', 'predicted pddl', 'gt pddl')\n",
    "                for p, v in pddl.items():\n",
    "                    log += '%15s|%15s|%15s\\n'%(p, predicted_pddl[i][p], v)\n",
    "                log += '-'*50 + '\\n\\n\\n'\n",
    "with open('log-roberta.txt', 'w') as f:\n",
    "    f.write(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make input.txt for eval trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = []\n",
    "for ep in os.listdir(os.path.join('data/json_2.1.0_Recep:original-gpt2/valid_seen')):\n",
    "    if os.path.isfile(os.path.join('data/json_2.1.0_Recep:original-gpt2/valid_seen', ep)):\n",
    "        continue\n",
    "    for trial in os.listdir(os.path.join('data/json_2.1.0_Recep:original-gpt2/valid_seen', ep)):\n",
    "        input.append(os.path.join('.', ep, trial))\n",
    "        \n",
    "with open(os.path.join('data/json_2.1.0_Recep:original-gpt2/valid_seen', 'input.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('found_valid_seen-recep:0.3.txt', 'r') as f:\n",
    "    l = f.readlines()\n",
    "l = list(set(l))\n",
    "with open('found_valid_seen-recep:0.3.txt', 'w') as f:\n",
    "    f.write(''.join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alfred unity simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ai2thor.server.Event at 0x7f6415f427c0\n",
       "    .metadata[\"lastAction\"] = RotateRight\n",
       "    .metadata[\"lastActionSuccess\"] = True\n",
       "    .metadata[\"errorMessage\"] = \"\n",
       "    .metadata[\"actionReturn\"] = None\n",
       ">"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ai2thor.controller import Controller\n",
    "\n",
    "controller = Controller(\n",
    "    agentMode=\"locobot\",\n",
    "    visibilityDistance=1.5,\n",
    "    scene=\"FloorPlan_Train1_3\",\n",
    "    gridSize=0.25,\n",
    "    movementGaussianSigma=0.005,\n",
    "    rotateStepDegrees=90,\n",
    "    rotateGaussianSigma=0.5,\n",
    "    renderDepthImage=False,\n",
    "    renderInstanceSegmentation=False,\n",
    "    width=300,\n",
    "    height=300,\n",
    "    fieldOfView=60\n",
    ")\n",
    "controller.reset(scene='FloorPlan_Train7_5', rotateStepDegrees=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.step(action='MoveAhead')\n",
    "positions = controller.step(action='GetReachablePositions').metadata['actionReturn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "events\n",
    "> metadata, frame, depth, instance_segmentation, instance_mask (+ bounding box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = controller.step(action='rotateLeft')\n",
    "# or\n",
    "event = controller.last_event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai2thor.util.metrics import (\n",
    "    get_shortest_path_to_object_type,\n",
    "    path_distance,\n",
    "    compute_single_spl\n",
    ")\n",
    "\n",
    "path = get_shortest_path_to_object_type(\n",
    "    controller=controller,\n",
    "    object_type='Apple',\n",
    "    initial_position=dict(\n",
    "        x=0.0,\n",
    "        y=0.9,\n",
    "        z=0.25\n",
    "    )\n",
    ")\n",
    "\n",
    "path_distance(path)\n",
    "compute_single_spl(path, shortest_path, successful_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
